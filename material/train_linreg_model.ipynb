{
 "cells": [
  {
   "source": [
    "MIT License\n",
    "\n",
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "This notebook is adapted from Francesca Lazzeri Energy Demand Forecast Workbench workshop.\n",
    "\n",
    "Copyright (c) 2021 PyLadies Amsterdam, Alyona Galyeva"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression with recursive feature elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from azureml.core import Workspace, Dataset\n",
    "from azureml.core.experiment import Experiment\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to train a linear regression model to create a forecast of future energy demand. In particular, the model will be trained to predict energy demand in period $t_{+1}$, one hour ahead of the current time period $t$. This is known as 'one-step' time series forecasting because we are predicting one period into the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKDIR = os.getcwd()\n",
    "MODEL_NAME = \"linear_regression\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset.get_by_name(ws, name=\"train_nyc_demand_data\")\n",
    "print(train_ds.name, train_ds.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_ds.to_pandas_dataframe()\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create design matrix - each column in this matrix represents a model feature and each row is a training example. We remove the *demand* and *timeStamp* variables as they are not model features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(['demand', 'timeStamp'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_experiment = Experiment(ws, name=\"LR\")\n",
    "run = lr_experiment.start_logging()\n",
    "\n",
    "run.log(\"dataset name\", train_ds.name)\n",
    "run.log(\"dataset version\", train_ds.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create predictive model pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use sklearn's Pipeline functionality to create a predictive model pipeline. For this model, the pipeline implements the following steps:\n",
    "- **one-hot encode categorical variables** - this creates a feature for each unique value of a categorical feature. For example, the feature *dayofweek* has 7 unique values. This feature is split into 7 individual features dayofweek0, dayofweek1, ... , dayofweek6. The value of these features is 1 if the timeStamp corresponds to that day of the week, otherwise it is 0.\n",
    "- **recursive feature elimination with cross validation (RFECV)** - it is often the case that some features add little predictive power to a model and may even make the model accuracy worse. Recursive feature elimination tests the model accuracy on increasingly smaller subsets of the features to identify the subset which produces the most accurate model. Cross validation is used to test each subset on multiple folds of the input data. The best model is that which achieves the lowest mean squared error averaged across the cross validation folds.\n",
    "- **train final model** - the best model found in after the feature elimination process is used to train the final estimator on the whole dataset.\n",
    "\n",
    "Identify indices for categorical columns for one hot encoding and create the OneHotEncoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['hour', 'month', 'dayofweek']\n",
    "cat_cols_idx = [X.columns.get_loc(c) for c in X.columns if c in cat_cols]\n",
    "run.log_list(\"cat_cols\", cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer([('encoder', OneHotEncoder(sparse=False), cat_cols_idx)], remainder='passthrough')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the linear regression model object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = LinearRegression(fit_intercept=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For hyperparameter tuning and feature selection, cross validation will be performed using the training set. With time series forecasting, it is important that test data comes from a later time period than the training data. This also applies to each fold in cross validation. Therefore a time series split is used to create three folds for cross validation as illustrated below. Each time series plot represents a separate training/test split, with the training set coloured in blue and the test set coloured in red. Note that, even in the first split, the training data covers at least a full year so that the model can learn the annual seasonality of the demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand_ts = train[['timeStamp', 'demand']].copy()\n",
    "demand_ts.reset_index(drop=True, inplace=True)\n",
    "\n",
    "for split_num, split_idx  in enumerate(tscv.split(demand_ts)):\n",
    "    split_num = str(split_num)\n",
    "    train_idx = split_idx[0]\n",
    "    test_idx = split_idx[1]\n",
    "    demand_ts['fold' + split_num] = \"not used\"\n",
    "    demand_ts.loc[train_idx, 'fold' + split_num] = \"train\"\n",
    "    demand_ts.loc[test_idx, 'fold' + split_num] = \"test\"\n",
    "\n",
    "gs = gridspec.GridSpec(3,1)\n",
    "fig = plt.figure(figsize=(15, 10), tight_layout=True)\n",
    "\n",
    "ax = fig.add_subplot(gs[0])\n",
    "ax.plot(demand_ts.loc[demand_ts['fold0']==\"train\", \"timeStamp\"], demand_ts.loc[demand_ts['fold0']==\"train\", \"demand\"], color='b')\n",
    "ax.plot(demand_ts.loc[demand_ts['fold0']==\"test\", \"timeStamp\"], demand_ts.loc[demand_ts['fold0']==\"test\", \"demand\"], 'r')\n",
    "ax.plot(demand_ts.loc[demand_ts['fold0']==\"not used\", \"timeStamp\"], demand_ts.loc[demand_ts['fold0']==\"not used\", \"demand\"], 'w')\n",
    "\n",
    "ax = fig.add_subplot(gs[1], sharex=ax)\n",
    "plt.plot(demand_ts.loc[demand_ts['fold1']==\"train\", \"timeStamp\"], demand_ts.loc[demand_ts['fold1']==\"train\", \"demand\"], 'b')\n",
    "plt.plot(demand_ts.loc[demand_ts['fold1']==\"test\", \"timeStamp\"], demand_ts.loc[demand_ts['fold1']==\"test\", \"demand\"], 'r')\n",
    "plt.plot(demand_ts.loc[demand_ts['fold1']==\"not used\", \"timeStamp\"], demand_ts.loc[demand_ts['fold1']==\"not used\", \"demand\"], 'w')\n",
    "\n",
    "ax = fig.add_subplot(gs[2], sharex=ax)\n",
    "plt.plot(demand_ts.loc[demand_ts['fold2']==\"train\", \"timeStamp\"], demand_ts.loc[demand_ts['fold2']==\"train\", \"demand\"], 'b')\n",
    "plt.plot(demand_ts.loc[demand_ts['fold2']==\"test\", \"timeStamp\"], demand_ts.loc[demand_ts['fold2']==\"test\", \"demand\"], 'r')\n",
    "plt.plot(demand_ts.loc[demand_ts['fold2']==\"not used\", \"timeStamp\"], demand_ts.loc[demand_ts['fold2']==\"not used\", \"demand\"], 'w')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the RFECV object. Note the metric for evaluating the model on each fold is the negative mean squared error. The best model is that which maximises this metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_cv = RFECV(estimator=regr,\n",
    "             cv=tscv,\n",
    "             scoring='neg_mean_squared_error',\n",
    "             verbose=2,\n",
    "             n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model pipeline object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_pipe = Pipeline([('preprocessor', preprocessor), ('rfecv', regr_cv)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model pipeline. This should only take a few seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_pipe.fit(X, y=train['demand'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log(\"pipeline steps\", regr_pipe.named_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the trained model pipeline object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(WORKDIR, MODEL_NAME + '.pkl'), 'wb') as f:\n",
    "    pickle.dump(regr_pipe, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore cross validation results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best CV negative mean squared error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log(\"best CV negMSE\", max(regr_pipe.named_steps['rfecv'].grid_scores_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Plot the cross validation errors with each subset of features. The chart shows that most features are useful to the model. However, the error gets significantly worse when there are 44 features or less in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv_results = pd.DataFrame.from_dict({'cv_score': regr_pipe.named_steps['rfecv'].grid_scores_})\n",
    "cv_results['mean_squared_error'] = cv_results['cv_score']\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(cv_results.index, cv_results['mean_squared_error'])\n",
    "plt.xlabel('number of features')\n",
    "plt.title('CV negative mean squared error')\n",
    "run.log_image(\"CV errors plot\", plot=plt)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of features selected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_pipe.named_steps['rfecv'].n_features_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify supported features after selection process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_onehot_cols(X):\n",
    "    X_dummy_cols = list(pd.get_dummies(X.copy()[cat_cols], columns=cat_cols).columns)\n",
    "    other_cols = list(X.columns.drop(cat_cols))\n",
    "    return X_dummy_cols + other_cols\n",
    "\n",
    "supported_features = pd.DataFrame.from_dict(\n",
    "    {'feature':get_onehot_cols(X), \n",
    "     'supported':regr_pipe.named_steps['rfecv'].support_}\n",
    ")\n",
    "supported_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect model coefficients for each included feature. The magnitude and direction of the coefficients indicates the effect the features has on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coefs = supported_features.loc[supported_features['supported'], ].copy()\n",
    "coefs['coefficients'] = regr_pipe.named_steps['rfecv'].estimator_.coef_\n",
    "coefs.plot.bar('feature', 'coefficients', figsize=(15, 3), legend=False)\n",
    "run.log_image(\"LR coefs per feature\", plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.complete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('mlops_train': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "15eb2d70be7d7d9baddaf6c9c4fecdefddeb65a737952d883f3b0e583de09784"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}