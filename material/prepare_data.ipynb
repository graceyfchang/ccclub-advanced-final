{
 "cells": [
  {
   "source": [
    "MIT License\n",
    "\n",
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "This notebook is adapted from [Francesca Lazzeri Energy Demand Forecast Workbench workshop](https://github.com/FrancescaLazzeri/EnergyDemandForecastWorkbench).\n",
    "\n",
    "Copyright (c) 2021 PyLadies Amsterdam, Alyona Galyeva\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "Run this notebook to prepare the data for modelling and registering prepared datasets.\n",
    "\n",
    "**Important prerequisites**: \n",
    "- Change the kernel to Python 3.8.10...('mlops_train':conda) You can do this from the *Kernel* menu under *Change kernel*.\n",
    "\n",
    "Run each cell of this notebook to perform the following steps:\n",
    "- Import the data from csv files and merge the two datasets.\n",
    "- The data is cleaned by filling gaps in the time series and handling missing values.\n",
    "- The data is explored through visualisation.\n",
    "- Features for the forecasting models are computed.\n",
    "- Data is split into training and test sets.\n",
    "- Connect to Azure Machine learning workspace and register processed, train, test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from azureml.core import Workspace, Dataset\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load energy demand data and merge with weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKDIR = os.getcwd()\n",
    "DATADIR = os.path.join(WORKDIR, 'datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand_raw = pd.read_csv(os.path.join(DATADIR,'nyc_demand_raw.csv'), parse_dates=['timeStamp'])\n",
    "weather_raw = pd.read_csv(os.path.join(DATADIR,'nyc_weather_raw.csv'), parse_dates=['timeStamp'])\n",
    "demand = pd.merge(demand_raw, weather_raw, on=['timeStamp'], how='outer')\n",
    "demand['demand'] = demand['demand'].astype(float)\n",
    "demand.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_df = demand.loc[(demand.timeStamp>'2016-07-01') & (demand.timeStamp<='2016-07-07')]\n",
    "plt.plot(plt_df['timeStamp'], plt_df['demand'])\n",
    "plt.title('New York City power demand over one week in July 2017')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill gaps in the time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some periods in the time series are missing. This occurs if the period was missing in both the original demand and weather datasets. To identify these gaps, first we create an index of time periods that we would *expect* to be in the time series. There should be one record for every hour between the minimum and maximum datetimes in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_time = min(demand['timeStamp'])\n",
    "min_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_time = max(demand['timeStamp'])\n",
    "max_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_idx = pd.date_range(min_time, max_time, freq='H')\n",
    "dt_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we index the dataframe according to this datetime index to insert missing records into the time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand.index = demand['timeStamp']\n",
    "demand = demand.reindex(dt_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inserted missing records will have NaN/NaT values for all columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand[demand.isnull().all(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that there are no missing periods in the time series, we can start handling missing values by filling as many many as possible. Firstly, count the number of missing values in each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing timeStamp can be filled from the dataframe index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand.loc[demand.isnull().all(axis=1), 'timeStamp'] = demand.loc[demand.isnull().all(axis=1)].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the other columns, we can fill many missing values by interpolating between the two closest non-missing values. Here, we use a quadratic function and set a limit of 6. This limit means that if more than 6 missing values occur consecutively, the missing values are not interpolated over and they remain missing. This is to avoid spurious interpolation between very distant time periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand = demand.interpolate(limit=6, method='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill missing precip values with common value of 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precip_mode = np.ndarray.item(stats.mode(demand['precip']).mode)\n",
    "demand['precip'] = demand['precip'].fillna(precip_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of missing values has now been greatly reduced. Records containing the remaining missing values will be removed later after model features have been created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By visualising the data, we can gain some intuition as to what kind of features could be helpful to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(demand['demand'].dropna(), bins=100)\n",
    "plt.title('Demand distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_df = demand.copy().loc[(demand['timeStamp']>='2016-01-01') & (demand['timeStamp']<'2017-01-01'), ]\n",
    "plt.plot(plt_df['timeStamp'], plt_df['demand'], markersize=1)\n",
    "plt.title('Hourly demand in 2016')\n",
    "plt.ylabel('demand')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(demand['temp'].dropna(), bins=100)\n",
    "plt.title('Temperature distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(demand['temp'], demand['demand'], 'ro', markersize=1)\n",
    "plt.title('Demand vs temperature')\n",
    "plt.xlabel('temp')\n",
    "plt.ylabel('demand')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The autocorrelation plot below shows the extent to which the demand variable correlates with itself at different intervals (lags). This plot shows that demand is highly autocorrelated over the closest 6 hour period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autocorrelation_plot(demand['demand'].dropna())\n",
    "plt.xlim(0,24)\n",
    "plt.title('Auto-correlation of hourly demand over a 24 hour period')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute features for forecasting models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After exploring the data, it is clear that the energy demand follows seasonal trends, with daily, weekly and annual periodicity. We will create features that encode this information. First, we compute time driven features based on timeStamp. Note for dayofweek, Monday=0 and Sunday=6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand_features = demand.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand_features['hour'] = demand_features.timeStamp.dt.hour\n",
    "demand_features['month'] = demand_features.timeStamp.dt.month-1\n",
    "demand_features['dayofweek'] = demand_features.timeStamp.dt.dayofweek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Compute lagged demand features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lagged_features(df, var, max_lag):\n",
    "    for t in range(1, max_lag+1):\n",
    "        df[var+'_lag'+str(t)] = df[var].shift(t, freq='1H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_lagged_features(demand_features, 'temp', 6)\n",
    "generate_lagged_features(demand_features, 'demand', 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final data cleaning and write out training and test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count remaining null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand_features.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count number of rows with any null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand_features.loc[demand_features.isnull().any(axis=1), ].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very small proportion of the overall dataset so can be safely dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand_features.dropna(how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand_features.to_csv(os.path.join(DATADIR, 'nyc_demand_processed.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into training and test datasets. All data after 1st July 2016 is reserved for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = (demand_features.loc[demand_features['timeStamp']<'2016-07-01'], demand_features.loc[demand_features['timeStamp']>='2016-07-01'])\n",
    "train.to_csv(os.path.join(DATADIR, 'nyc_demand_train.csv'), float_format='%.4f', index=False)\n",
    "test.to_csv(os.path.join(DATADIR, 'nyc_demand_test.csv'), float_format='%.4f', index=False)"
   ]
  },
  {
   "source": [
    "### Register datasets to the workspace"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the workspace from config.json\n",
    "ws = Workspace.from_config()\n",
    "# get the datastore to upload our data\n",
    "datastore = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore.upload(src_dir='datasets', target_path='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = Dataset.Tabular.from_delimited_files(datastore.path('data/nyc_demand_processed.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview the first 3 rows of the dataset from datastore\n",
    "processed_dataset.take(3).to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_ds = processed_dataset.register(workspace=ws, name=\"processed_nyc_demand_data\", description=\"processed New York energy demand data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.Tabular.from_delimited_files(datastore.path('data/nyc_demand_train.csv'))\n",
    "test_dataset = Dataset.Tabular.from_delimited_files(datastore.path('data/nyc_demand_test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.take(3).to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.take(3).to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_dataset.register(workspace=ws, name=\"train_nyc_demand_data\", description=\"processed New York energy demand data for training\")\n",
    "test_ds = test_dataset.register(workspace=ws, name=\"test_nyc_demand_data\", description=\"processed New York energy demand data for testing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('mlops_train': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "15eb2d70be7d7d9baddaf6c9c4fecdefddeb65a737952d883f3b0e583de09784"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}